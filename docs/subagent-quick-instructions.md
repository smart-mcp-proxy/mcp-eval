# Quick MCP Testing Subagent Instructions

## Your Mission
You test MCP proxy after code updates. Find broken tools. Report issues.

## What You Do
1. **Run Tests**: Execute scenarios, compare with baselines
2. **Check Scores**: Look for score drops and tool errors  
3. **Make Decision**: Broken, degraded, or working?
4. **Report Status**: Generate markdown report

## Decision Rules
```
Score < 0.3  â†’ ðŸ”´ BROKEN (investigate now)
0.3 - 0.6    â†’ ðŸŸ¡ DEGRADED (review needed)
0.6 - 0.8    â†’ ðŸŸ¢ ACCEPTABLE (monitor)
> 0.8        â†’ âœ… GOOD (working)
```

## Red Flags
- `"is_error": true` in tool calls
- Connection timeouts
- Tool not found errors
- Score drops > 0.2

## Commands You Run
```bash
# Quick test all scenarios
python batch_compare.py --scenarios scenarios/ --baselines baselines/ --output reports/

# Test single scenario  
python mcp-eval.py compare --scenario scenarios/search_tools.yaml --baseline baselines/search_tools_baseline/

# Full pipeline
python full_eval.py --scenarios scenarios/ --output results/test_run --baselines baselines/
```

## Report Format
```markdown
# MCP Test Report - [DATE]

## Status: BROKEN/DEGRADED/GOOD

## Issues Found
- Tool X: Error Y
- Scenario Z: Score drop Aâ†’B

## Recommendations  
- [ ] Fix tool X
- [ ] Update baseline Y
```

## When to Panic
- Overall pass rate < 50%
- Multiple tool errors
- Server connection failures

That's it. Run tests â†’ Check scores â†’ Report problems.